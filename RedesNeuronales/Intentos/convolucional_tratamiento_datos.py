# -*- coding: utf-8 -*-
"""Convolucional_tratamiento_datos.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v5SbpD850keczRAZxVTMm6KRyUs6iKRR
"""

import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import time

from sklearn.feature_selection import VarianceThreshold
import matplotlib.pyplot as plt
from sklearn.preprocessing import PolynomialFeatures

# the data, split between train and test sets
(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

# Scale images to the [0, 1] range
x_train = x_train.astype("float32") / 255
x_test = x_test.astype("float32") / 255

x_train = np.reshape(x_train,[60000,784])
x_test = np.reshape(x_test,[10000,784])

sel = VarianceThreshold(threshold=(0.01))
sel.fit_transform(x_train)
array = sel.get_support()
print('\nLas partes negras corresponten a caracteristicas eliminadas')
plt.imshow(array.reshape(28,28), cmap='gray')
plt.show()


sel.fit(x_train)
x_train = sel.transform(x_train)
x_test = sel.transform(x_test)
print("x_train shape:", x_train.shape)
print("x_test shape:", x_test.shape)

#poly = PolynomialFeatures(1)
#x_train= poly.fit_transform(x_train) 
#print("x_train shape:", x_train.shape)

#x_train = np.resize(x_train, [60000, 484])
#x_test = np.resize(x_test, [60000, 484])

#x_train = np.resize(x_train, [60000, 22, 22])
#x_test = np.resize(x_test, [60000, 22, 22])

#print(x_train.shape, "train samples")
#print(x_test.shape, "test samples")

x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)
print("x_train shape:", x_train.shape)
print(x_train.shape[0], "train samples")
print(x_test.shape[0], "test samples")

# Model / data parameters
num_classes = 10
input_shape = (444, 1)

# convert class vectors to binary class matrices
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

model = keras.Sequential(
    [
        keras.Input(shape=input_shape),
        layers.Conv1D(32, kernel_size=(3), activation="relu"),
        layers.MaxPooling1D(pool_size=(3)),
        layers.Conv1D(64, kernel_size=(3), activation="relu"),
        layers.MaxPooling1D(pool_size=(3)),
        layers.Flatten(),
        layers.Dropout(0.2),
        layers.Dense(64, activation="relu"),
        layers.Dropout(0.2),
        layers.Dense(num_classes, activation="softmax"),
    ]
)

model.summary()

batch_size = 128
epochs = 25

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

start = time.time()
model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)
end = time.time()

score_test = model.evaluate(x_test, y_test, verbose=0)
score_train = model.evaluate(x_train, y_train, verbose=0)
outputs = model.predict(x_test)

for output in outputs:
  print(np.argmax(output), end='')

print("")
print("Error sobre el conjunto de prueba:", (1-score_test[1])*100)
print("Error sobre el conjunto de entrenamiento:", (1-score_train[1])*100)
print("Tiempo de entrenamiento: "+str(round(end - start,2)))
print("Epoch:", epochs)